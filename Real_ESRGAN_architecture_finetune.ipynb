{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rEO7yNFPQ_n"
      },
      "source": [
        "**General**\n",
        "For ease of use, we recommend downloading the original Real-ESRGAN reposetory (https://github.com/xinntao/Real-ESRGAN), and uploading the folder to a google colab.\n",
        "In any case, in order to run this notebook it is required to put it int the Real-ESRGAN folder.\n",
        "\n",
        "The notebook is comprosed of 2 sections, training and inference. If only one is required, skip the irrelevent part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pecZo3rA7nPP",
        "outputId": "b522ae8b-7ad9-4ace-e99c-adbe9b75e92a"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "# !cp -r /content/drive/MyDrive/Real-ESRGAN-master/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e60xnG7NO4F4",
        "outputId": "4fff551b-d26a-4573-e39d-3521343aa390"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Install basicsr - https://github.com/xinntao/BasicSR\n",
        "# We use BasicSR for both training and inference\n",
        "!pip install basicsr\n",
        "# facexlib and gfpgan are for face enhancement\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install piqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have low quality (LQ) images at the input and high quality images (HQ) at the output. HQ images are used as ground-truth. \n",
        "\n",
        "The input images must have 256x16 and the output 1024x64.\n",
        "\n",
        "For training, we used the sequence 00 from Semantic-Kitti dataset, with 9k images.\n",
        "\n",
        "The scheme path to the directories are:\n",
        "\n",
        "- datasets/\n",
        "    - meta_info_pair.txt\n",
        "    - input\n",
        "        - splited_images\n",
        "            - 00\n",
        "            - test\n",
        "    - output\n",
        "        - original_images\n",
        "\n",
        "The follow steps are in [github.com/xinntao/Real-ESRGAN/blob/master/docs/Training.md](https://github.com/xinntao/Real-ESRGAN/blob/master/docs/Training.md#computer-how-to-trainfinetune-real-esrgan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = \"/mnt/d/Dev/Projects/Lume/super_resolution/datasets/\"\n",
        "input_path = os.path.join(f\"{dataset_path}\",\"input\",\"splited_images\") # LQ\n",
        "output_path = os.path.join(f\"{dataset_path}\",\"output\",\"original_imgs\") # HQ\n",
        "\n",
        "# Directories for train, test, and validation splits\n",
        "train_dir = os.path.join(f\"{dataset_path}\", \"train\")\n",
        "test_dir_hq = os.path.join(f\"{dataset_path}\",\"test\" ,\"hq\")\n",
        "test_dir_lq = os.path.join(f\"{dataset_path}\",\"test\" ,\"lq\")\n",
        "val_dir_hq = os.path.join(f\"{dataset_path}\", \"validation\", \"hq\")\n",
        "val_dir_lq = os.path.join(f\"{dataset_path}\", \"validation\", \"lq\")\n",
        "\n",
        "# Createthe directorys if they not exists\n",
        "if not os.path.exists(train_dir):\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "if not os.path.exists(val_dir):\n",
        "    os.makedirs(val_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename the files in the output folder so that they have the same file name as the input folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# rename validation files name\n",
        "def rename_files(directory):\n",
        "    # List all files in the specified directory\n",
        "    files = os.listdir(directory)\n",
        "    print(f\"path {directory}\")\n",
        "    for file in files:\n",
        "        # Check if the file name contains \"_original\"\n",
        "        print(f\"checking file: {file}\")\n",
        "        if '_original' in file:\n",
        "            # Generate the new file name by removing \"_original\"\n",
        "            new_name = file.replace('_original', '')\n",
        "\n",
        "            # Rename the file\n",
        "            os.rename(os.path.join(directory, file), os.path.join(directory, new_name))\n",
        "\n",
        "\n",
        "# Rename the files\n",
        "rename_files(val_dir_hq)\n",
        "rename_files(val_dir_lq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if you want to organize the dataset directories to the format: datasets > input > 00 > 00_image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def organize_files(base_directory):\n",
        "    # List all files in the base directory\n",
        "    files = os.listdir(base_directory)\n",
        "    print(f\"Organizando {base_directory}...\")\n",
        "\n",
        "    for file in files:\n",
        "        # Skip directories, we only want to move files\n",
        "        if os.path.isfile(os.path.join(base_directory, file)):\n",
        "            # Extract the prefix (first two characters) from the filename\n",
        "            prefix = file[:2]\n",
        "            print(f\"Ordem {prefix}...\")\n",
        "\n",
        "            # Create a new directory with the prefix if it doesn't already exist\n",
        "            new_directory = os.path.join(base_directory, prefix)\n",
        "            if not os.path.exists(new_directory):\n",
        "                os.makedirs(new_directory)\n",
        "\n",
        "            # Move the file to the new directory\n",
        "            shutil.move(os.path.join(base_directory, file), os.path.join(new_directory, file))\n",
        "\n",
        "# Organize the files\n",
        "organize_files(input_path)\n",
        "organize_files(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you wanna organize the directory with all the images on train, test and validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file_list = os.listdir(input_path)\n",
        "\n",
        "# Split the dataset into feature and target sets\n",
        "train_files, test_files = train_test_split(file_list, test_size=0.2, random_state=42)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OajYr-bK3Fh"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRG7pjhkTRcV"
      },
      "source": [
        "To use the validation, we must create lower resultion versions of the same images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Originally, for HQ images we had dimensions of 1024x64 and 1024x16 for LQ images. To use this model, we must have images with a dimension that is divisible by 4, and when upscaling the scale, corresponds to the dimension of the HQ image. To do this, we resize the LQ images by processing and dividing the image through columns of pixels,\n",
        "using the file **split_images.py**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FINETUNE\n",
        "\n",
        "As we already have both directorys (input LQ and output HQ), we'll finetuning the model with our own paired data. For this, we will use pair directories as input and output, and the config file meta_train_pair.txt.\n",
        "\n",
        "The original Real-ESRGAN repo requires to create a meta info file for the train dataset. This cell will run a script to generate exactly that.\n",
        "\n",
        "The following steps are originally from https://github.com/xinntao/Real-ESRGAN/blob/master/docs/Training.md#use-your-own-paired-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwNzCSSKhPhb",
        "outputId": "16bd7a37-9737-498c-9f96-df0ac2f5086b"
      },
      "outputs": [],
      "source": [
        "# Create meta info file for pair datasets\n",
        "!python scripts/generate_meta_info_pairdata.py --input /datasets/output/original_imgs/00 /datasets/input/splited_images/00 --meta_info /datasets/meta_info_train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz_pLNo9VEXd"
      },
      "source": [
        "Here we shall download the original Real-ESRGAN weights. If you are going to work on different weights, you can skip this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z539YU0-AAIT",
        "outputId": "626be9c1-6ee1-4861-8f2c-7cffe3bff267"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.3/RealESRGAN_x4plus_netD.pth -P experiments/pretrained_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjpt8tIXVcNV"
      },
      "source": [
        "And finally, run this cell in order to strat the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvl7pjcKAhL8",
        "outputId": "8047cfeb-d687-41b4-bbb2-7deca43fe54d"
      },
      "outputs": [],
      "source": [
        "!python realesrgan/train_finetune_architecture.py -opt options/finetune_realesrgan_x4plus_pairdata.yml --auto_resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjLONC2HWJcV"
      },
      "source": [
        "We will also need to copy the new weights to the wegihts folder from the experiment folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIv65X6cWRTT"
      },
      "outputs": [],
      "source": [
        "# !cp experiments/finetune_RealESRGANx4plus_400k/models/net_g_latest.pth weights/net_g_latest.pth\n",
        "!cp /experiments/finetune_RealESRGANx4plus_400k_pairdata/models/net_g_latest.pth weights/net_g_latest.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6HAhkNaVnCD"
      },
      "source": [
        "Now we shall test the trained model. Similiar to the validation set, we first must create the lower resoultion versions of the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn8wFJdqLPFF"
      },
      "source": [
        "**Inference and Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "!cp -v /mnt/d/Dev/Projects/Lume/super_resolution/datasets/output/original_imgs/01/*.png /mnt/d/Dev/Projects/Lume/super_resolution/datasets/test/hq/ # 11k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C82vqfR0Weur"
      },
      "source": [
        "Finally, we can run the inference script in order ro get the test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfp9thOOY_xo",
        "outputId": "db8bebf9-0631-4d09-ef71-00a0703c1fbc"
      },
      "outputs": [],
      "source": [
        "# !python inference_realesrgan_architecture.py -n net_g_latest.pth -i inputs_test --outscale 2 --output new_model_results\n",
        "!python inference_realesrgan.py -n net_g_latest.pth -i /datasets/input/splited_images/01/test --outscale 4 --output new_model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEuSwb4DBeF4"
      },
      "source": [
        "Or we can infer with previosly fine tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHFCmEBQgS6h",
        "outputId": "498273eb-c4e9-41df-ecbb-4914c33b2ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "net_g_architecture\n",
            "weights/net_g_architecture.pth\n",
            "4\n",
            "Testing 0 1_000097\n",
            "Testing 1 1_000119\n",
            "Testing 2 2_000097\n",
            "Testing 3 2_000119\n",
            "Testing 4 3_000097\n",
            "Testing 5 3_000119\n",
            "Testing 6 4_000097\n",
            "Testing 7 4_000119\n"
          ]
        }
      ],
      "source": [
        "# !python inference_realesrgan.py -n net_g_architecture.pth -i /mnt/d/Dev/Projects/Lume/super_resolution/datasets/input/splited_images/01/test --outscale 4 --output /mnt/d/Dev/Projects/Lume/super_resolution/datasets/input/splited_images/01/results\n",
        "!python inference_realesrgan.py -n net_g_architecture.pth -i /datasets/input/splited_images/01/LR_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KGAVm6jWoD7"
      },
      "source": [
        "If there is a need, you could run the cell below in order to use the original Real-ESRGAN weights, and to test them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_wdjW0gYQq",
        "outputId": "82312b38-539f-4933-c4c1-a108e1422af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "Testing 0 downscale_F176_VOX_arquitectura_Beam_\n",
            "Testing 1 downscale_F176_VOX_arquitectura_Door_Beam_Handrail_\n",
            "Testing 2 downscale_F176_VOX_arquitectura_Drawings_\n",
            "Testing 3 downscale_F176_VOX_arquitectura_Facade_Handrail_\n",
            "Testing 4 downscale_F176_VOX_arquitectura_Facade_Windows_\n",
            "Testing 5 downscale_F176_VOX_arquitectura_Fence_Facade_Handrail_\n",
            "Testing 6 downscale_F176_VOX_arquitectura_Stairs_Handrail_Steel_\n",
            "Testing 7 downscale_F176_VOX_arquitectura_Windows_\n",
            "Testing 8 downscale_F176_VOX_arquitectura_Windows_Brick_Facade_\n",
            "Testing 9 downscale_F176_VOX_arquitectura_Windows_Facade_\n",
            "Testing 10 downscale_F176_VOX_arquitectura_Windows_Facade_Handrail_\n",
            "Testing 11 downscale_F176_VOX_arquitectura_Windows_Fence_Glass_Chair_Handrail_\n",
            "Testing 12 downscale_F176_VOX_arquitectura_Windows_Stairs_Facade_Beam_Handrail_\n",
            "Testing 13 downscale_F35_House_Projetebem_Arquitetura_Drawings_\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/inference_realesrgan_architecture.py\", line 170, in <module>\n",
            "    main()\n",
            "  File \"/content/inference_realesrgan_architecture.py\", line 151, in main\n",
            "    output, _ = upsampler.enhance(img, outscale=args.outscale)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/realesrgan/utils.py\", line 223, in enhance\n",
            "    self.process()\n",
            "  File \"/content/realesrgan/utils.py\", line 115, in process\n",
            "    self.output = self.model(self.img)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/basicsr/archs/rrdbnet_arch.py\", line 113, in forward\n",
            "    body_feat = self.conv_body(self.body(feat))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/basicsr/archs/rrdbnet_arch.py\", line 59, in forward\n",
            "    out = self.rdb1(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/basicsr/archs/rrdbnet_arch.py\", line 34, in forward\n",
            "    x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python inference_realesrgan_architecture.py -n RealESRGAN_x4plus -i inputs_test --outscale 2 --output golden_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVqqDra3XP7S"
      },
      "source": [
        "If there is a need to comapre metric results, modify this code to your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QXXHr5rlXOe",
        "outputId": "969bae02-c0e7-48d3-f215-a42d3f32593b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg_our_niqe 4.36796694945366 avg_orig_niqe  3.40678806614249 avg_gold_niqe 4.437186537028811\n",
            "avg_our_psnr tensor(21.8031) avg_gold_psnr  tensor(22.2831) avg_gt_psnr tensor(70.)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from basicsr.metrics import calculate_ssim, calculate_psnr, calculate_niqe\n",
        "import torch\n",
        "import piqa\n",
        "from os.path import exists\n",
        "psnr = piqa.PSNR()\n",
        "ssim = piqa.ssim.SSIM()\n",
        "avg_our_niqe = 0.0\n",
        "avg_orig_niqe = 0.0\n",
        "avg_gold_niqe = 0.0\n",
        "avg_our_psnr = 0.0\n",
        "avg_gold_psnr = 0.0\n",
        "avg_gt_psnr = 0.0\n",
        "total_files = 0\n",
        "for filename in glob.glob('datasets/modernarchitecture/test/hq/*.jpg'):\n",
        "    head, tail = os.path.split(filename)\n",
        "    golden_result = Image.open('golden_results/downscale_'+tail[:-4]+'_out.jpg')\n",
        "    our_result = Image.open('our_results/downscale_'+tail[:-4]+'_out.jpg')\n",
        "    gt = Image.open(filename)\n",
        "    our_result = our_result.resize((gt.size[0], gt.size[1]))\n",
        "    golden_result = golden_result.resize((gt.size[0], gt.size[1]))\n",
        "    transform = transforms.PILToTensor()\n",
        "    golden_result = transform(golden_result)\n",
        "    our_result = transform(our_result)\n",
        "    gt = transform(gt)\n",
        "    if(our_result.numpy().shape[0] == 3 and gt.numpy().shape == our_result.numpy().shape and gt.numpy().shape == golden_result.numpy().shape):\n",
        "      avg_our_niqe += calculate_niqe(img = our_result.numpy(), crop_border=4, test_y_channel=False, input_order='CHW' ,convert_to='gray')\n",
        "      avg_orig_niqe += calculate_niqe(img = gt.numpy(), crop_border=4, test_y_channel=False, input_order='CHW' ,convert_to='gray')\n",
        "      avg_gold_niqe += calculate_niqe(img = golden_result.numpy(), crop_border=4, test_y_channel=False, input_order='CHW' ,convert_to='gray')\n",
        "      avg_our_psnr += psnr(our_result.type(torch.float)/255, gt.type(torch.float)/255)\n",
        "      avg_gold_psnr += psnr(golden_result.type(torch.float)/255, gt.type(torch.float)/255)\n",
        "      avg_gt_psnr += psnr(gt.type(torch.float)/255, gt.type(torch.float)/255)\n",
        "    total_files += 1\n",
        "avg_our_niqe /= total_files\n",
        "avg_orig_niqe /= total_files\n",
        "avg_gold_niqe /= total_files\n",
        "avg_our_psnr /= total_files\n",
        "avg_gold_psnr /= total_files\n",
        "avg_gt_psnr /= total_files\n",
        "print(\"avg_our_niqe\", avg_our_niqe, \"avg_orig_niqe \", avg_orig_niqe, \"avg_gold_niqe\", avg_gold_niqe)\n",
        "print(\"avg_our_psnr\", avg_our_psnr, \"avg_gold_psnr \", avg_gold_psnr, \"avg_gt_psnr\", avg_gt_psnr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC2uApLjY-HS"
      },
      "source": [
        "Here is an example of how to cut a snippet of the images to focus on specific parts of an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dlt0_4Cwx1n"
      },
      "outputs": [],
      "source": [
        "!mkdir patches/\n",
        "from PIL import Image\n",
        "import glob\n",
        "ext = \".jpg\"\n",
        "og_filename = \"F176_VOX_arquitectura_Beam_\"\n",
        "changed_filename = \"downscale_\" + og_filename + \"_out\"\n",
        "images = []\n",
        "catgories = [\"our\", \"golden\", \"og\"]\n",
        "for category in catgories:\n",
        "  if category == \"og\":\n",
        "    image = Image.open(\"datasets/modernarchitecture/test/hq/\" + og_filename + \".jpg\")\n",
        "  else:\n",
        "    image = Image.open(category + \"_results/\" + changed_filename + \".jpg\")\n",
        "\n",
        "  width, height = image.size\n",
        "  left = 0\n",
        "  top = 0\n",
        "  right = width / 3\n",
        "  bottom = height / 6\n",
        "\n",
        "  image = image.crop((left, top, right, bottom))\n",
        "  images.append(image)\n",
        "\n",
        "  image.save(\"patches/patch_\" + category + \"_\" + og_filename + ext)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
